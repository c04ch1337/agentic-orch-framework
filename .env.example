# LLM Service Configuration

# Default Provider: OpenRouter
LLM_API_URL=https://openrouter.ai/api/v1/chat/completions
LLM_API_KEY=your_openrouter_key_here
LLM_MODEL=anthropic/claude-3.5-sonnet

# --- Provider Options (Uncomment to use) ---

# OpenAI
# LLM_API_URL=https://api.openai.com/v1/chat/completions
# LLM_API_KEY=your_openai_key_here
# LLM_MODEL=gpt-4-turbo

# Grok (xAI)
# LLM_API_URL=https://api.x.ai/v1/chat/completions
# LLM_API_KEY=your_grok_key_here
# LLM_MODEL=grok-1

# Gemini (Google)
# LLM_API_URL=https://generativelanguage.googleapis.com/v1beta/openai/chat/completions
# LLM_API_KEY=your_gemini_key_here
# LLM_MODEL=gemini-1.5-pro

# Ollama (Local)
# LLM_API_URL=http://localhost:11434/v1/chat/completions
# LLM_API_KEY=ollama
# LLM_MODEL=llama3

# LMStudio (Local)
# LLM_API_URL=http://localhost:1234/v1/chat/completions
# LLM_API_KEY=lm-studio
# LLM_MODEL=local-model

# ============================================================
# Docker Port Configuration
# Modify these values to resolve port conflicts on your host
# Internal container ports remain fixed at gRPC standards
# ============================================================

# Core Services
ORCHESTRATOR_PORT=50051
DATA_ROUTER_PORT=50052
LLM_SERVICE_PORT=50053
TOOLS_SERVICE_PORT=50054
SAFETY_SERVICE_PORT=50055
LOGGING_SERVICE_PORT=50056

# Knowledge Bases
MIND_KB_PORT=50057
BODY_KB_PORT=50058
HEART_KB_PORT=50059
SOCIAL_KB_PORT=50060
SOUL_KB_PORT=50061

# Phase 4 Services
EXECUTOR_PORT=50062

# Phase 5 Services
CONTEXT_MANAGER_PORT=50064
REFLECTION_PORT=50065
SCHEDULER_PORT=50066
AGENT_REGISTRY_PORT=50067

# Phase 6 Agent Services
RED_TEAM_PORT=50068
BLUE_TEAM_PORT=50069

# API Gateway
API_GATEWAY_PORT=8282

# Logging
RUST_LOG=info
